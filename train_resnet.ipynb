{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet50-based model for material property prediction.\n",
    "This module implements a ResNet50-based model for predicting material properties.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_least_utilized_gpu():\n",
    "    \"\"\"Get the GPU with the least memory usage.\"\"\"\n",
    "    try:\n",
    "        # Run nvidia-smi to get GPU memory usage\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "                              capture_output=True, text=True)\n",
    "        memory_usage = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "        \n",
    "        # Find GPU with minimum memory usage\n",
    "        min_usage = min(memory_usage)\n",
    "        gpu_id = memory_usage.index(min_usage)\n",
    "        \n",
    "        print(f\"Selected GPU {gpu_id} with {min_usage}MB memory usage\")\n",
    "        return gpu_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting GPU usage: {e}\")\n",
    "        return 0  # Fallback to GPU 0 if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaterialDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "    \n",
    "class MaterialPropertyMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(MaterialPropertyMLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(ResNet50Model, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify the first layer to accept our input dimension\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Modify the final layer for our regression task\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input to match ResNet50's expected input shape\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 1, -1, 1)  # Reshape to [batch_size, 1, features, 1]\n",
    "        x = x.expand(-1, 1, -1, 224)  # Expand to match ResNet50's expected size\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_resnet50(X_train, X_test, y_train, y_test, target='band_gap', \n",
    "                  batch_size=32, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train a ResNet50-based model for material property prediction.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Training and testing features\n",
    "        y_train, y_test: Training and testing targets\n",
    "        target: Target property name\n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \"\"\"\n",
    "    # Select the least utilized GPU\n",
    "    gpu_id = get_least_utilized_gpu()\n",
    "    device = torch.device(f'cuda:{gpu_id}')\n",
    "    print(f\"Using GPU {gpu_id} for training\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Create datasets\n",
    "    train_dataset = MaterialDataset(X_train, y_train)\n",
    "    test_dataset = MaterialDataset(X_test, y_test)\n",
    "    \n",
    "    # Create data loaders with multiple workers\n",
    "    num_workers = 8#min(4, os.cpu_count() or 1)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ResNet50Model(input_dim=X_train.shape[1])\n",
    "    #model = MaterialPropertyMLP(input_dim=X_train.shape[1])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Create progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # Create progress bar for batches\n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        \n",
    "        for inputs, targets in batch_pbar:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            print(torch.cuda.memory_summary())\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'batch_loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                test_loss += criterion(outputs, targets).item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'test_loss': f'{test_loss:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f'models/resnet50_{target}.pth')\n",
    "    \n",
    "    return model, train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_resnet50(model, X):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained ResNet50 model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ResNet50 model\n",
    "        X: Input features\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    gpu_id = get_least_utilized_gpu()\n",
    "    device = torch.device(f'cuda:{gpu_id}')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X = torch.FloatTensor(X).to(device)\n",
    "        predictions = model(X)\n",
    "    \n",
    "    return predictions.cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to train the ResNet50 model for material property prediction.\n",
    "This script uses the existing data preprocessing pipeline and trains the ResNet50 model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_elements_to_features(elements_str):\n",
    "    \"\"\"Convert elements string into numeric features\"\"\"\n",
    "    # Split elements and count occurrences\n",
    "    elements = elements_str.split(',')\n",
    "    element_counts = Counter(elements)\n",
    "    \n",
    "    # Create a dictionary of element counts\n",
    "    features = {}\n",
    "    for element in element_counts:\n",
    "        features[f'element_{element}'] = element_counts[element]\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(target='band_gap'):\n",
    "    \"\"\"Load and preprocess data for ResNet50 model\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(\"materials_for_ml.csv\")\n",
    "    \n",
    "    # Drop rows with missing target values\n",
    "    df = df.dropna(subset=[target])\n",
    "    \n",
    "    # Convert elements to numeric features\n",
    "    element_features = pd.DataFrame([convert_elements_to_features(elem) for elem in df['formula']])\n",
    "    element_features = element_features.fillna(0)  # Fill NaN with 0 for elements not present\n",
    "    \n",
    "    # Select features\n",
    "    categorical_cols = [\"crystal_system\"]\n",
    "    element_cols = [col for col in df.columns if col.startswith('contains_')]\n",
    "    numerical_cols = [col for col in df.columns if col not in categorical_cols + element_cols + [target, 'material_id', 'formula']]\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df, columns=categorical_cols)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop(columns=[target, 'material_id', 'formula'])\n",
    "    \n",
    "    # Combine original features with element features\n",
    "    X = pd.concat([X, element_features], axis=1)\n",
    "    \n",
    "    y = df[target]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, f'models/resnet50_{target}_scaler.pkl')\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train.values, y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(train_losses, test_losses, target):\n",
    "    \"\"\"Plot training and validation loss history\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Validation Loss')\n",
    "    plt.title(f'ResNet50 Training History - {target}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots/resnet50_{target}_training_history.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Target properties to predict\n",
    "    targets = ['band_gap', 'formation_energy_per_atom']  # Add more targets as needed\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"\\nTraining ResNet50 model for {target}...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        X_train, X_test, y_train, y_test = load_and_preprocess_data(target)\n",
    "        \n",
    "        # Train model\n",
    "        model, train_losses, test_losses = train_resnet50(\n",
    "            X_train, X_test, y_train, y_test,\n",
    "            target=target,\n",
    "            batch_size=32,\n",
    "            num_epochs=5,\n",
    "            learning_rate=0.001\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(train_losses, test_losses, target)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = predict_resnet50(model, X_test)\n",
    "        \n",
    "        # Calculate and print metrics\n",
    "        mse = np.mean((y_test - y_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        \n",
    "        print(f\"\\nResults for {target}:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet50 model for band_gap...\n",
      "Selected GPU 0 with 2007MB memory usage\n",
      "Using GPU 0 for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 15884, 49660, 41672, 26056, 20740, 46112, 31948, 7908) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(target)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model, train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_resnet50\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[0;32m     21\u001b[0m plot_training_history(train_losses, test_losses, target)\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mtrain_resnet50\u001b[1;34m(X_train, X_test, y_train, y_test, target, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Create progress bar for batches\u001b[39;00m\n\u001b[0;32m     48\u001b[0m batch_pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m batch_pbar:\n\u001b[0;32m     51\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda(), targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     52\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1402\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1404\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Chirayu\\.conda\\envs\\tf-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 15884, 49660, 41672, 26056, 20740, 46112, 31948, 7908) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
