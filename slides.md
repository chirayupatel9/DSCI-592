---
layout: center
aspectRatio: 16/9
theme: default
---
# GPUâ€‘Accelerated Image EmbeddingÂ & Activeâ€‘Learning Pipeline

### A deepâ€‘dive walkthrough

Chirayu Patel Â· Anish Kania Â· Aryan Jain Â· Manav Bhagat
---

## Table of Contents

1. Project Overview
2. Data & Storage Strategy
3. Hardware & Environment
4. Model Architecture
5. Embedding Extraction
6. Dimensionality Reduction
7. Clustering & Classification
8. Retrieval System
9. Activeâ€‘Learning Loop
10. Results & Performance
11. Lessons Learned
12. Next Steps

---

## 1â€‚Projectâ€¯Overview

* **Goal:** Build a scalable pipeline that converts raw PNG images â†’ compact embeddings â†’ actionable insights.
* **Scale:** \~1â€¯million images (â‰ƒâ€¯800â€¯GB) across 17 target classes.
* **Key tech stack:** PyTorch, cuML, scikitâ€‘learn, LightGBM, FAISS (future).
* **Output:**

  * Interactive visual maps (tâ€‘SNE / UMAP / PCA)
  * Rapid image retrieval (KNN)
  * Iteratively improving classifier via active learning.

---

## 2â€‚DataÂ &Â Storage Strategy

* **Raw source:** Internal blob storage â†’ nightly sync to `data/raw/`.
* **Preâ€‘process:**

  ```bash
  convert *.tiff -resize 512x512 png24:data/output/png/%04d.png
  ```
* **Metadata:** Saved to **Parquet** for fast IO (<2â€¯s to load 1â€¯M rows).
* **Embeddings cache:** `embeddings.lmdb` (readâ€‘optimized; 1.4â€¯GB).
* **Why LMDB?**

  * Zeroâ€‘copy reads
  * Memoryâ€‘mapped â†’ low RAM footprint
  * Concurrent readers, single writer = ideal for offline batch jobs.

---

## 3â€‚Model Architecture

```mermaid
flowchart LR
    A[Input PNG 224Ã—224] -->|RGB| B(Convâ€‘7Ã—7,â€¯64)
    B --> C(BN + ReLU + MaxPool)
    C --> D(ResidualÂ Block Ã—Â 3)
    D --> E(ResidualÂ Block Ã—Â 4)
    E --> F(ResidualÂ Block Ã—Â 6)
    F --> G(ResidualÂ Block Ã—Â 3)
    G --> H(AvgPoolÂ +Â Flatten)
    H --> I[Custom FC Head]
    I -->|128â€‘D Embedding| J
```

**Custom FC Head**
`2048 â†’ BN â†’ Dropout(0.5) â†’ 512 â†’ ReLU â†’ BN â†’ Dropout â†’ 64 â†’ ReLU â†’ BN â†’ Dropout â†’ 17â€‘class logits`

---

## 4â€‚Embedding Extraction

```python
model.fc = model.fc[:-1]   # strip output layer â†’ 64â€‘D
model.eval()
embeddings, paths = [], []
for batch, fns in loader:
    with torch.no_grad():
        vec = model(batch.to("cuda:4"))
    embeddings.append(vec.cpu())
    paths += fns
embeddings = torch.cat(embeddings).numpy()  # shape: (1â€¯M,Â 64)
```

* **Throughput:** 9â€¯kâ€¯img/s on a single A100.
* **Bottleneck:** Disk â†’ GPU transfer (solved with preâ€‘fetch queue & pinned memory).

---

## 5â€‚Dimensionality Reduction

### a)Â tâ€‘SNEÂ (cuML)
![TSNE_Using_CUML](./plots/05152025Anomaly.png)
* **Perplexity:**Â 30
* **Iterations:**â€¯1â€¯000
* **Barnesâ€‘Hut GPU** acceleration â†’ 8â€¯min (vs 3â€¯h CPU).
---

### b)Â UMAPÂ (cuML)

![UMAP_Using_CUML](./plots/05152025cuML_UMAP.png)
* **n\_neighbors:**Â 15
* **min\_dist:**Â 0.1
* Completed in 90â€¯s.
---


### c)Â PCAÂ (scikitâ€‘learn, CPU)
![PCA_visualization](./plots/05152025PCA_Visualization.png)

* Centered & whitened; used mainly for quick sanity checks.

---

## 6â€‚Clustering

* **Algorithm:** Miniâ€‘Batch Kâ€‘Means (kâ€¯=â€¯10) for scalability.
* **Initialization:** kâ€‘means++ with 20 restarts.
* **Evaluation:** Silhouette scoreâ€¯â‰ˆâ€¯0.43 â†’ reasonable separation.

### Cluster Size Distribution

```csv
cluster,count
0,112â€¯456
1,97â€¯234
2,101â€¯890
3,93â€¯442
4,97â€¯811
5,104â€¯003
6,98â€¯770
7,94â€¯512
8,100â€¯205
9,99â€¯677
```


---
![Anomaly_Detection](./plots/05152025Anomaly.png)

---
![KNN](./plots/05152025neighbour.png)
---
## 7â€‚Classification

### Logistic Regression (baseline)

* **Input:** 64â€‘D embeddings
* **Solver:** `lbfgs`, max\_iterâ€¯=â€¯1â€¯000
* **Accuracy:**Â `79.6â€¯%` on heldâ€‘out 20â€¯%.

### LightGBM (GPU)

```python
params = {
  'objective':'multiclass', 'num_class':10,
  'learning_rate':0.05, 'num_leaves':255,
  'feature_fraction':0.9, 'device':'gpu'
}
```

* **Best multiâ€‘logloss:**Â 0.3123
* **Accuracy:**Â `86.2â€¯%` (â†‘â€¯6.6â€¯pp over baseline).

---

## 8â€‚Anomalyâ€¯Detection

* **Model:** IsolationÂ Forest (`n_estimators`â€¯=â€¯200).
* **Threshold:** Topâ€¯5â€¯% mostâ€‘isolated flagged.
* **Useâ€‘case:** Surface mislabeled or corrupted images quickly for manual review.

---

## 9â€‚Imageâ€¯Retrieval

```python
import faiss
index = faiss.IndexFlatL2(64)
index.add(embeddings.astype('float32'))
D, I = index.search(query_vec.astype('float32'), k=5)
```

* **Latency:** <â€¯5â€¯ms per query (inâ€‘RAM).
* **Plan:** Persist as IVFâ€‘PQ for billionâ€‘scale.

---

## 10â€‚Activeâ€‘Learning Strategy

1. **Seed set:** RandomÂ 5â€¯% labeled.
2. **Model:** LightGBM on current labeled pool.
3. **Uncertainty:** 1Â â€“Â maxÂ probability (entropy also tested).
4. **Query batch:** TopÂ 5â€¯% most uncertain â†’ annotate â†’ add to pool.
5. **Stop:** Until validation accuracy plateaus.

### Pseudocode

```python
for t in range(T):
    clf.fit(X_lab, y_lab)
    probs = clf.predict_proba(X_unlab)
    uncertainty = 1 - probs.max(1)
    Q = uncertainty.argsort()[-Q_size:]
    X_lab = np.vstack([X_lab, X_unlab[Q]])
    y_lab = np.hstack([y_lab, y_unlab[Q]])
    X_unlab = np.delete(X_unlab, Q, axis=0)
```

---

## 11â€‚Activeâ€‘Learning Results

* **Start:**Â 79â€¯%
* **After 10 iterations:**Â 92.4â€¯% (+13â€¯pp).
* Labeled set grew from 5â€¯% â†’ 55â€¯% (but guided by uncertainty).

---

## 12â€‚Performance Profile

| Component             | Wallâ€‘time    | GPU Util | Peak VRAM |
| --------------------- | ------------ | -------- | --------- |
| Embedding extraction  | **1â€¯hâ€¯54â€¯m** | 92â€¯%     | 11â€¯GB     |
| tâ€‘SNE (cuML)          | â€¯12â€¯s     | 80â€¯%     | 6â€¯GB      |
| UMAP (cuML)           |  29â€¯s     | 68â€¯%     | 4â€¯GB      |
| Kâ€‘Means (MB)          | 3â€¯mâ€¯40â€¯s     | 10â€¯%     | 0.5â€¯GB    |
| LightGBM (100â€¯rounds) | 2 h 35 m     | 90â€¯%      | 35â€¯GB      |

---

## 13â€‚Lessonsâ€¯Learned

* **I/O trumps FLOPs:** Proper dataÂ loader prefetch doubled throughput.
* **cuML quirks:** Ensure matching **CUDA toolkit** versions or segfaults.
* **Class imbalance:** Address via stratified sampling before clustering.
* **Active learning**: Uncertainty sampling > random but annotation cost grows; consider costâ€‘sensitive query.

---

## 14â€‚Nextâ€¯Steps

* Hyperparameter search with **Optuna** on embeddings & LightGBM.
* Swap backbone to **DINOv2 ViTâ€‘L**; compare selfâ€‘supervised vs supervised.
* Replace KNN with **FAISS IVFâ€‘PQ** for subâ€‘ms retrieval.
* Deploy inference via **Triton** with NVIDIAÂ A2 nodes for costâ€‘effective scaling.

---

## 15â€‚AppendixÂ A â€” LibraryÂ Versions

```text
Python            3.10.14
PyTorch           2.2.1+cu124
cuML              24.02.00
scikitâ€‘learn      1.5.0
LightGBM          4.3.0 (GPU)
CUDA Toolkit      12.4
FAISS             1.8.0
```


---

# ThankÂ you ğŸ™

## <small>Questions & Discussion</small>
