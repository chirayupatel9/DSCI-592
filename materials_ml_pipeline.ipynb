{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25040cd",
   "metadata": {},
   "source": [
    "# Materials Science Machine Learning Pipeline\n",
    "\n",
    "This notebook contains the complete implementation of the materials science machine learning pipeline, including data processing, model training, API implementation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96b639",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea3852",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0cf79c",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a24b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, mongo_uri: str = 'mongodb://localhost:27017/'):\n",
    "        self.client = MongoClient(mongo_uri)\n",
    "        self.db = self.client['materials_db']\n",
    "        self.collection = self.db['materials']\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load data from MongoDB\"\"\"\n",
    "        try:\n",
    "            data = pd.DataFrame(list(self.collection.find()))\n",
    "            logger.info(f\"Loaded {len(data)} records from MongoDB\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Preprocess the materials data\"\"\"\n",
    "        try:\n",
    "            # Handle missing values\n",
    "            df = df.dropna(subset=['band_gap', 'formation_energy'])\n",
    "            \n",
    "            # Feature engineering\n",
    "            df['avg_atomic_radius'] = df['atomic_radii'].apply(lambda x: np.mean(x))\n",
    "            df['total_electrons'] = df['element_counts'].apply(lambda x: sum(x.values()))\n",
    "            \n",
    "            # Select features and target\n",
    "            features = ['avg_atomic_radius', 'total_electrons', 'crystal_system', 'space_group']\n",
    "            target = 'band_gap'\n",
    "            \n",
    "            # Convert categorical variables\n",
    "            df = pd.get_dummies(df, columns=['crystal_system', 'space_group'])\n",
    "            \n",
    "            return df[features], df[target]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_processed_data(self, df: pd.DataFrame, filename: str):\n",
    "        \"\"\"Save processed data to file\"\"\"\n",
    "        try:\n",
    "            df.to_csv(filename, index=False)\n",
    "            logger.info(f\"Saved processed data to {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958db04",
   "metadata": {},
   "source": [
    "## 3. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc4548",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.best_params = {}\n",
    "        \n",
    "    def train_random_forest(self, X: pd.DataFrame, y: pd.Series) -> RandomForestRegressor:\n",
    "        \"\"\"Train Random Forest model with hyperparameter tuning\"\"\"\n",
    "        try:\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "            \n",
    "            rf = RandomForestRegressor(random_state=42)\n",
    "            grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2')\n",
    "            grid_search.fit(X, y)\n",
    "            \n",
    "            self.best_params['random_forest'] = grid_search.best_params_\n",
    "            self.models['random_forest'] = grid_search.best_estimator_\n",
    "            \n",
    "            logger.info(f\"Random Forest best parameters: {grid_search.best_params_}\")\n",
    "            return grid_search.best_estimator_\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training Random Forest: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def train_gradient_boosting(self, X: pd.DataFrame, y: pd.Series) -> GradientBoostingRegressor:\n",
    "        \"\"\"Train Gradient Boosting model with hyperparameter tuning\"\"\"\n",
    "        try:\n",
    "            param_grid = {\n",
    "                'n_estimators': [200, 500, 1000],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 6, 9]\n",
    "            }\n",
    "            \n",
    "            gb = GradientBoostingRegressor(random_state=42)\n",
    "            grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='r2')\n",
    "            grid_search.fit(X, y)\n",
    "            \n",
    "            self.best_params['gradient_boosting'] = grid_search.best_params_\n",
    "            self.models['gradient_boosting'] = grid_search.best_estimator_\n",
    "            \n",
    "            logger.info(f\"Gradient Boosting best parameters: {grid_search.best_params_}\")\n",
    "            return grid_search.best_estimator_\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training Gradient Boosting: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_model(self, model, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics = {\n",
    "                'r2': r2_score(y_test, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'mae': np.mean(np.abs(y_test - y_pred))\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Model metrics: {metrics}\")\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def save_model(self, model, filename: str):\n",
    "        \"\"\"Save trained model to file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            logger.info(f\"Saved model to {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def load_model(self, filename: str):\n",
    "        \"\"\"Load trained model from file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            logger.info(f\"Loaded model from {filename}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735df3d3",
   "metadata": {},
   "source": [
    "## 4. API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f947b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PredictionAPI:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.app = Flask(__name__)\n",
    "        self.model = self.load_model(model_path)\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        \"\"\"Setup API routes\"\"\"\n",
    "        @self.app.route('/predict', methods=['POST'])\n",
    "        def predict():\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                features = self.preprocess_input(data)\n",
    "                prediction = self.model.predict(features)\n",
    "                \n",
    "                response = {\n",
    "                    'prediction': float(prediction[0]),\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'model_version': '1.0.0'\n",
    "                }\n",
    "                \n",
    "                return jsonify(response)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error making prediction: {str(e)}\")\n",
    "                return jsonify({'error': str(e)}), 400\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            return jsonify({'status': 'healthy'})\n",
    "    \n",
    "    def preprocess_input(self, data: dict) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess input data for prediction\"\"\"\n",
    "        try:\n",
    "            # Convert input data to DataFrame\n",
    "            df = pd.DataFrame([data])\n",
    "            \n",
    "            # Apply same preprocessing as training\n",
    "            df['avg_atomic_radius'] = df['atomic_radii'].apply(lambda x: np.mean(x))\n",
    "            df['total_electrons'] = df['element_counts'].apply(lambda x: sum(x.values()))\n",
    "            \n",
    "            # Convert categorical variables\n",
    "            df = pd.get_dummies(df, columns=['crystal_system', 'space_group'])\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing input: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, host: str = '0.0.0.0', port: int = 5000):\n",
    "        \"\"\"Run the API server\"\"\"\n",
    "        self.app.run(host=host, port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c9852",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6171e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self, output_dir: str = 'plots'):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def plot_feature_importance(self, model, feature_names: List[str], title: str):\n",
    "        \"\"\"Plot feature importance\"\"\"\n",
    "        importance = model.feature_importances_\n",
    "        indices = np.argsort(importance)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(title)\n",
    "        plt.bar(range(len(importance)), importance[indices])\n",
    "        plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{title.lower().replace(\" \", \"_\")}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_actual_vs_predicted(self, y_test: pd.Series, y_pred: np.ndarray, title: str):\n",
    "        \"\"\"Plot actual vs predicted values\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{title.lower().replace(\" \", \"_\")}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_crystal_system_distribution(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot distribution of crystal systems\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        df['crystal_system'].value_counts().plot(kind='bar')\n",
    "        plt.title('Crystal System Distribution')\n",
    "        plt.xlabel('Crystal System')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'crystal_system_distribution.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_band_gap_distribution(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot distribution of band gaps\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['band_gap'], bins=30)\n",
    "        plt.title('Band Gap Distribution')\n",
    "        plt.xlabel('Band Gap (eV)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'band_gap_distribution.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_element_distribution(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot distribution of elements\"\"\"\n",
    "        element_counts = {}\n",
    "        for counts in df['element_counts']:\n",
    "            for element, count in counts.items():\n",
    "                if element in element_counts:\n",
    "                    element_counts[element] += count\n",
    "                else:\n",
    "                    element_counts[element] = count\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        pd.Series(element_counts).sort_values(ascending=False).head(20).plot(kind='bar')\n",
    "        plt.title('Top 20 Elements Distribution')\n",
    "        plt.xlabel('Element')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'element_distribution.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_property_correlation(self, df: pd.DataFrame):\n",
    "        \"\"\"Plot correlation between properties\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df[['band_gap', 'formation_energy', 'avg_atomic_radius', 'total_electrons']].corr(),\n",
    "                    annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Property Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'property_correlation.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_error_distribution(self, y_test: pd.Series, y_pred: np.ndarray, title: str):\n",
    "        \"\"\"Plot error distribution\"\"\"\n",
    "        errors = y_test - y_pred\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(errors, bins=30)\n",
    "        plt.title(f'{title} Error Distribution')\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, f'{title.lower().replace(\" \", \"_\")}_error_distribution.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad822bc",
   "metadata": {},
   "source": [
    "## 6. Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89de3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize components\n",
    "    data_processor = DataProcessor()\n",
    "    model_trainer = ModelTrainer()\n",
    "    visualizer = Visualizer()\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    logger.info(\"Loading and preprocessing data...\")\n",
    "    df = data_processor.load_data()\n",
    "    X, y = data_processor.preprocess_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    logger.info(\"Training Random Forest...\")\n",
    "    rf_model = model_trainer.train_random_forest(X_train, y_train)\n",
    "    \n",
    "    logger.info(\"Training Gradient Boosting...\")\n",
    "    gb_model = model_trainer.train_gradient_boosting(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    logger.info(\"Evaluating models...\")\n",
    "    rf_metrics = model_trainer.evaluate_model(rf_model, X_test, y_test)\n",
    "    gb_metrics = model_trainer.evaluate_model(gb_model, X_test, y_test)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    y_pred_gb = gb_model.predict(X_test)\n",
    "    \n",
    "    # Create visualizations\n",
    "    logger.info(\"Generating visualizations...\")\n",
    "    visualizer.plot_feature_importance(rf_model, X.columns, 'Random Forest Feature Importance')\n",
    "    visualizer.plot_feature_importance(gb_model, X.columns, 'Gradient Boosting Feature Importance')\n",
    "    \n",
    "    visualizer.plot_actual_vs_predicted(y_test, y_pred_rf, 'Random Forest Predictions')\n",
    "    visualizer.plot_actual_vs_predicted(y_test, y_pred_gb, 'Gradient Boosting Predictions')\n",
    "    \n",
    "    visualizer.plot_crystal_system_distribution(df)\n",
    "    visualizer.plot_band_gap_distribution(df)\n",
    "    visualizer.plot_element_distribution(df)\n",
    "    visualizer.plot_property_correlation(df)\n",
    "    \n",
    "    visualizer.plot_error_distribution(y_test, y_pred_rf, 'Random Forest')\n",
    "    visualizer.plot_error_distribution(y_test, y_pred_gb, 'Gradient Boosting')\n",
    "    \n",
    "    # Save models\n",
    "    logger.info(\"Saving models...\")\n",
    "    model_trainer.save_model(rf_model, 'models/random_forest.pkl')\n",
    "    model_trainer.save_model(gb_model, 'models/gradient_boosting.pkl')\n",
    "    \n",
    "    # Initialize and run API\n",
    "    logger.info(\"Starting API server...\")\n",
    "    api = PredictionAPI('models/random_forest.pkl')\n",
    "    api.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
